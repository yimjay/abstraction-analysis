#define max(a,b) (((a) < (b))? (b) : (a))
#define min(a,b) (((a) < (b))? (a) : (b))

#include <SSE3Dnow.h>

#include <stdlib.h>
#include <omp.h>


void dsyr2k(const int M,const int N,const int K,const double alpha,const double *A,const int lda,const double *B,const int ldb,const double beta,double *C,const int ldc)
{
  int i; 
  int j;
  int k;
{
   int j_bk_1;
   int j_bk_2;
   int i_bk_3;
   int k_bk_4;
   int B_cp_j_bk_6_index;
   int B_cp_j_bk_2_bk_5_index;
   int B_cp_k_bk_7_index;
   int B_cp_index;
   double* B_cp_alloc;
   double* B_cp;
   int A_cp_j_bk_9_index;
   int A_cp_j_bk_2_bk_8_index;
   int A_cp_k_bk_10_index;
   int A_cp_index;
   double* A_cp_alloc;
   double* A_cp;
   double _B_cp_0_0;
   double _B_cp_1_0;
   double _B_cp__n_0_0;
   double _B_cp__n_0_1;
   double _B_cp__n_1_0;
   double _B_cp__n_1_1;
   double _A_cp_0_0;
   double _A_cp_1_0;
   double _A_cp__n_0_0;
   double _A_cp__n_1_0;
   double _C_0_0;
   double _C_0_1;
   double _C_1_0;
   double _C_1_1;
   double _tmp__split;
   double* __FD__B_cp_0;
   double* __FD__B_cp_0_0;
   double* __FD__B_cp_0_0_0;
   double* __FD__B_cp_0_0_0_0;
   double* __FD__B_cp__n_0;
   double* __FD__B_cp__n_0_0;
   double* __FD__B_cp__n_0_0_0;
   double* __FD__B_cp__n_0_0_0_0;
   double* __FD__A_cp_0;
   double* __FD__A_cp_0_0;
   double* __FD__A_cp_0_0_0;
   double* __FD__A_cp_0_0_0_0;
   double* __FD__A_cp__n_0;
   double* __FD__A_cp__n_0_0;
   double* __FD__A_cp__n_0_0_0;
   double* __FD__A_cp__n_0_0_0_0;
   double* __FD__C_0;
   double* __FD__C_0_0;
   double* __FD__C_0_0_0;
   double* __FD__C_0_0_0_0;
   B_cp_alloc=(double*)malloc((64*((63+N)/64)*(128*((127+N)/128))+(1<<16))*sizeof(double));
   B_cp=(double*)((size_t)B_cp_alloc + (1 << 16) >> 16 << 16);
   B_cp_index = 0;
   for (B_cp_j_bk_6_index=0; B_cp_j_bk_6_index<-127+N; B_cp_j_bk_6_index+=128)
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<128+B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
          }
     }
   if (B_cp_j_bk_6_index<N) 
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<-63+N; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_6_index+128-B_cp_k_bk_7_index)*64;
          }
        if (B_cp_j_bk_2_bk_5_index<N) 
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_2_bk_5_index+64-B_cp_k_bk_7_index)*64;
             B_cp_j_bk_2_bk_5_index = B_cp_j_bk_2_bk_5_index+64;
          }
        B_cp_index = B_cp_index+(B_cp_j_bk_6_index+128-B_cp_j_bk_2_bk_5_index)*64;
        B_cp_j_bk_6_index = B_cp_j_bk_6_index+128;
     }
   A_cp_alloc=(double*)malloc((64*((63+N)/64)*(128*((127+N)/128))+(1<<16))*sizeof(double));
   A_cp=(double*)((size_t)A_cp_alloc + (1 << 16) >> 16 << 16);
   A_cp_index = 0;
   for (A_cp_j_bk_9_index=0; A_cp_j_bk_9_index<-127+N; A_cp_j_bk_9_index+=128)
     {
        for (A_cp_j_bk_2_bk_8_index=A_cp_j_bk_9_index; A_cp_j_bk_2_bk_8_index<128+A_cp_j_bk_9_index; A_cp_j_bk_2_bk_8_index+=64)
          {
             for (A_cp_k_bk_10_index=0; A_cp_k_bk_10_index<-63+N; A_cp_k_bk_10_index+=64)
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<64+A_cp_j_bk_2_bk_8_index; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<64+A_cp_k_bk_10_index; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                    }
               }
             if (A_cp_k_bk_10_index<N) 
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<64+A_cp_j_bk_2_bk_8_index; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<N; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                       A_cp_index = A_cp_index+(-N+(64+A_cp_k_bk_10_index))*1;
                    }
                  A_cp_k_bk_10_index = A_cp_k_bk_10_index+64;
               }
          }
     }
   if (A_cp_j_bk_9_index<N) 
     {
        for (A_cp_j_bk_2_bk_8_index=A_cp_j_bk_9_index; A_cp_j_bk_2_bk_8_index<-63+N; A_cp_j_bk_2_bk_8_index+=64)
          {
             for (A_cp_k_bk_10_index=0; A_cp_k_bk_10_index<-63+N; A_cp_k_bk_10_index+=64)
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<64+A_cp_j_bk_2_bk_8_index; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<64+A_cp_k_bk_10_index; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                    }
               }
             if (A_cp_k_bk_10_index<N) 
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<64+A_cp_j_bk_2_bk_8_index; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<N; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                       A_cp_index = A_cp_index+(-N+(64+A_cp_k_bk_10_index))*1;
                    }
                  A_cp_k_bk_10_index = A_cp_k_bk_10_index+64;
               }
             A_cp_index = A_cp_index+(A_cp_j_bk_9_index+128-A_cp_k_bk_10_index)*64;
          }
        if (A_cp_j_bk_2_bk_8_index<N) 
          {
             for (A_cp_k_bk_10_index=0; A_cp_k_bk_10_index<-63+N; A_cp_k_bk_10_index+=64)
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<N; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<64+A_cp_k_bk_10_index; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                    }
                  A_cp_index = A_cp_index+(-N+(64+A_cp_j_bk_2_bk_8_index))*64;
               }
             if (A_cp_k_bk_10_index<N) 
               {
                  for (j=A_cp_j_bk_2_bk_8_index; j<N; j+=1)
                    {
                       for (k=A_cp_k_bk_10_index; k<N; k+=1)
                         {
                            A_cp[A_cp_index++] = A[j+k*lda];
                         }
                       A_cp_index = A_cp_index+(-N+(64+A_cp_k_bk_10_index))*1;
                    }
                  A_cp_index = A_cp_index+(-N+(64+A_cp_j_bk_2_bk_8_index))*64;
                  A_cp_k_bk_10_index = A_cp_k_bk_10_index+64;
               }
             A_cp_index = A_cp_index+(A_cp_j_bk_2_bk_8_index+64-A_cp_k_bk_10_index)*64;
             A_cp_j_bk_2_bk_8_index = A_cp_j_bk_2_bk_8_index+64;
          }
        A_cp_index = A_cp_index+(A_cp_j_bk_9_index+128-A_cp_j_bk_2_bk_8_index)*64;
        A_cp_j_bk_9_index = A_cp_j_bk_9_index+128;
     }
   omp_set_num_threads(2);
   #pragma omp  parallel  
    {
    #pragma omp  for private(__FD__C_0,__FD__C_0_0,__FD__C_0_0_0,__FD__C_0_0_0_0,__FD__A_cp__n_0,__FD__A_cp__n_0_0,__FD__A_cp__n_0_0_0,__FD__A_cp__n_0_0_0_0,__FD__A_cp_0,__FD__A_cp_0_0,__FD__A_cp_0_0_0,__FD__A_cp_0_0_0_0,__FD__B_cp__n_0,__FD__B_cp__n_0_0,__FD__B_cp__n_0_0_0,__FD__B_cp__n_0_0_0_0,__FD__B_cp_0,__FD__B_cp_0_0,__FD__B_cp_0_0_0,__FD__B_cp_0_0_0_0,_C_0_0,_C_0_1,_C_1_0,_C_1_1,_A_cp__n_0_0,_A_cp__n_1_0,_A_cp_0_0,_A_cp_1_0,_B_cp__n_0_0,_B_cp__n_0_1,_B_cp__n_1_0,_B_cp__n_1_1,_B_cp_0_0,_B_cp_1_0,k,i,j,j_bk_1,j_bk_2,i_bk_3,k_bk_4,_tmp__split)
    for (j_bk_1=0; j_bk_1<N; j_bk_1+=128)
      {
         __FD__B_cp_0 = B_cp+j_bk_1*B_cp_k_bk_7_index;
         __FD__B_cp__n_0 = B_cp+j_bk_1*B_cp_k_bk_7_index;
         __FD__A_cp_0 = A_cp+j_bk_1*A_cp_k_bk_10_index;
         __FD__C_0 = C+j_bk_1*ldc;
         for (j_bk_2=0; j_bk_2<-63+min(128,N-j_bk_1); j_bk_2+=64)
           {
              __FD__B_cp__n_0_0 = __FD__B_cp__n_0;
              __FD__A_cp__n_0 = A_cp;
              __FD__C_0_0 = __FD__C_0;
              for (i_bk_3=0; i_bk_3<-63+N; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-63+N; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg3);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+(1+ldc),reg4);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(1+__FD__B_cp__n_0_0_0_0,reg10);
                                  vec_splat(64+__FD__B_cp__n_0_0_0_0,reg11);
                                  vec_splat(65+__FD__B_cp__n_0_0_0_0,reg12);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg2,reg15);
                                  vec_mov_rm_1(reg2,1+__FD__C_0_0_0_0);
                                  vec_red(reg3,reg15);
                                  vec_mov_rm_1(reg3,__FD__C_0_0_0_0+ldc);
                                  vec_red(reg4,reg15);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+(1+ldc));
                                  __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0_0+2;
                                  __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*2;
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<N) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg3);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+(1+ldc),reg4);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(1+__FD__B_cp__n_0_0_0_0,reg10);
                                  vec_splat(64+__FD__B_cp__n_0_0_0_0,reg11);
                                  vec_splat(65+__FD__B_cp__n_0_0_0_0,reg12);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-3+min(64,N-k_bk_4); k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-1+min(64,N-k_bk_4); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<N-k_bk_4; k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg5);
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+(64+k),reg6);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+(64+k),reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg13);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+k),reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg12,reg0);
                                       vec_add_rr(reg0,reg4);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg4);
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg2,reg15);
                                  vec_mov_rm_1(reg2,1+__FD__C_0_0_0_0);
                                  vec_red(reg3,reg15);
                                  vec_mov_rm_1(reg3,__FD__C_0_0_0_0+ldc);
                                  vec_red(reg4,reg15);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+(1+ldc));
                                  __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0_0+2;
                                  __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*2;
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_10_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              if (i_bk_3<N) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-63+N; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg3);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(64+__FD__B_cp__n_0_0_0_0,reg11);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg15);
                                  vec_mov_rm_1(reg3,__FD__C_0_0_0_0+ldc);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*2;
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<N) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg3);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(64+__FD__B_cp__n_0_0_0_0,reg11);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-3+min(64,N-k_bk_4); k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-1+min(64,N-k_bk_4); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<N-k_bk_4; k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg5);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+(64+k),reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg13);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+k),reg14);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg11,reg0);
                                       vec_add_rr(reg0,reg3);
                                       vec_mov_rr(reg8,reg0);
                                       vec_mul_rr(reg14,reg0);
                                       vec_add_rr(reg0,reg3);
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg15);
                                  vec_mov_rm_1(reg3,__FD__C_0_0_0_0+ldc);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*2;
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_10_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_k_bk_7_index;
              __FD__B_cp__n_0 = __FD__B_cp__n_0+64*B_cp_k_bk_7_index;
              __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_10_index;
              __FD__C_0 = __FD__C_0+64*ldc;
           }
         if (j_bk_2<min(128,N-j_bk_1)) 
           {
              __FD__B_cp__n_0_0 = __FD__B_cp__n_0;
              __FD__A_cp__n_0 = A_cp;
              __FD__C_0_0 = __FD__C_0;
              for (i_bk_3=0; i_bk_3<-63+N; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-63+N; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(128-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg2);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(1+__FD__B_cp__n_0_0_0_0,reg10);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg2,reg15);
                                  vec_mov_rm_1(reg2,1+__FD__C_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0_0+2;
                                  __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<N) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(128-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg2);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  vec_splat(1+__FD__B_cp__n_0_0_0_0,reg10);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-3+min(64,N-k_bk_4); k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-1+min(64,N-k_bk_4); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(64+__FD__A_cp__n_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<N-k_bk_4; k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg5);
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+(64+k),reg6);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg6,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_add_rr(reg0,reg2);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg2);
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  vec_red(reg2,reg15);
                                  vec_mov_rm_1(reg2,1+__FD__C_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0_0+2;
                                  __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_10_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              if (i_bk_3<N) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-63+N; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(128-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<N) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(128-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg1);
                                  vec_splat(__FD__B_cp__n_0_0_0_0,reg9);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-3+min(64,N-k_bk_4); k+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-1+min(64,N-k_bk_4); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg5);
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<N-k_bk_4; k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg5);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg13);
                                       vec_mov_rr(reg5,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_add_rr(reg0,reg1);
                                       vec_mov_rr(reg7,reg0);
                                       vec_mul_rr(reg13,reg0);
                                       vec_add_rr(reg0,reg1);
                                    }
                                  vec_red(reg1,reg15);
                                  vec_mov_rm_1(reg1,__FD__C_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_10_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_k_bk_7_index;
              __FD__B_cp__n_0 = __FD__B_cp__n_0+64*B_cp_k_bk_7_index;
              __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_10_index;
              __FD__C_0 = __FD__C_0+64*ldc;
           }
      }
    }
   
   free(A_cp_alloc);
   free(B_cp_alloc);
}}

