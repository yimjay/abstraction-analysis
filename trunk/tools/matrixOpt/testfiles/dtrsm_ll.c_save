#define max(a,b) (((a) < (b))? (b) : (a))
#define min(a,b) (((a) < (b))? (a) : (b))

#include <SSE3Dnow.h>

#include <stdlib.h>
#include <omp.h>


void dtrsm_ll(const int M,const int N,const int K,const double alpha,const double *A,const int lda,const double *B1,const int ldb1,const double beta,double *B,const int ldb)
{
    long i,j,k;

{
   int j_bk_1;
   int j_bk_2;
   int i_bk_3;
   int k_bk_4;
   int B_cp_j_bk_6_index;
   int B_cp_j_bk_2_bk_5_index;
   int B_cp_k_bk_7_index;
   int B_cp_index;
   double* B_cp_alloc;
   double* B_cp;
   int A_cp_i_bk_8_index;
   int A_cp_k_bk_9_index;
   int A_cp_index;
   double* A_cp_alloc;
   double* A_cp;
   double _B_cp_0_0;
   double _B_cp_1_0;
   double _B_cp_2_0;
   double _B_cp_3_0;
   double _B_cp__n_0_0;
   double _B_cp__n_1_0;
   double _B_cp__n_2_0;
   double _B_cp__n_3_0;
   double _A_cp_0;
   double _A_cp__n_0_0;
   double _tmp__split;
   double* __FD__B_cp_0;
   double* __FD__B_cp_0_0;
   double* __FD__B_cp_0_0_0;
   double* __FD__B_cp_0_0_0_0;
   double* __FD__B_cp__n_0;
   double* __FD__B_cp__n_0_0;
   double* __FD__B_cp__n_0_0_0;
   double* __FD__B_cp__n_0_0_0_0;
   double* __FD__A_cp_0;
   double* __FD__A_cp_0_0;
   double* __FD__A_cp__n_0;
   double* __FD__A_cp__n_0_0;
   double* __FD__A_cp__n_0_0_0;
   double* __FD__A_cp__n_0_0_0_0;
   B_cp_alloc=(double*)malloc((64*((63+N)/64)*(256*((255+N)/256))+(1<<16))*sizeof(double));
   B_cp=(double*)((size_t)B_cp_alloc + (1 << 16) >> 16 << 16);
   B_cp_index = 0;
   for (B_cp_j_bk_6_index=0; B_cp_j_bk_6_index<-255+N; B_cp_j_bk_6_index+=256)
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<256+B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
          }
     }
   if (B_cp_j_bk_6_index<N) 
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<-63+N; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_6_index+256-B_cp_k_bk_7_index)*64;
          }
        if (B_cp_j_bk_2_bk_5_index<N) 
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B_cp[B_cp_index++] = B[k+j*ldb];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_2_bk_5_index+64-B_cp_k_bk_7_index)*64;
             B_cp_j_bk_2_bk_5_index = B_cp_j_bk_2_bk_5_index+64;
          }
        B_cp_index = B_cp_index+(B_cp_j_bk_6_index+256-B_cp_j_bk_2_bk_5_index)*64;
        B_cp_j_bk_6_index = B_cp_j_bk_6_index+256;
     }
   A_cp_alloc=(double*)malloc((64*((63+N)/64)*(64*((63+N)/64))+(1<<16))*sizeof(double));
   A_cp=(double*)((size_t)A_cp_alloc + (1 << 16) >> 16 << 16);
   A_cp_index = 0;
   for (A_cp_i_bk_8_index=0; A_cp_i_bk_8_index<-63+N; A_cp_i_bk_8_index+=64)
     {
        for (A_cp_k_bk_9_index=0; A_cp_k_bk_9_index<-63+N; A_cp_k_bk_9_index+=64)
          {
             for (i=A_cp_i_bk_8_index; i<64+A_cp_i_bk_8_index; i+=1)
               {
                  for (k=A_cp_k_bk_9_index; k<64+A_cp_k_bk_9_index; k+=1)
                    {
                       A_cp[A_cp_index++] = A[i+k*lda];
                    }
               }
          }
        if (A_cp_k_bk_9_index<N) 
          {
             for (i=A_cp_i_bk_8_index; i<64+A_cp_i_bk_8_index; i+=1)
               {
                  for (k=A_cp_k_bk_9_index; k<N; k+=1)
                    {
                       A_cp[A_cp_index++] = A[i+k*lda];
                    }
                  A_cp_index = A_cp_index+(-N+(64+A_cp_k_bk_9_index))*1;
               }
             A_cp_k_bk_9_index = A_cp_k_bk_9_index+64;
          }
     }
   if (A_cp_i_bk_8_index<N) 
     {
        for (A_cp_k_bk_9_index=0; A_cp_k_bk_9_index<-63+N; A_cp_k_bk_9_index+=64)
          {
             for (i=A_cp_i_bk_8_index; i<N; i+=1)
               {
                  for (k=A_cp_k_bk_9_index; k<64+A_cp_k_bk_9_index; k+=1)
                    {
                       A_cp[A_cp_index++] = A[i+k*lda];
                    }
               }
             A_cp_index = A_cp_index+(-N+(64+A_cp_i_bk_8_index))*64;
          }
        if (A_cp_k_bk_9_index<N) 
          {
             for (i=A_cp_i_bk_8_index; i<N; i+=1)
               {
                  for (k=A_cp_k_bk_9_index; k<N; k+=1)
                    {
                       A_cp[A_cp_index++] = A[i+k*lda];
                    }
                  A_cp_index = A_cp_index+(-N+(64+A_cp_k_bk_9_index))*1;
               }
             A_cp_index = A_cp_index+(-N+(64+A_cp_i_bk_8_index))*64;
             A_cp_k_bk_9_index = A_cp_k_bk_9_index+64;
          }
        A_cp_index = A_cp_index+(A_cp_i_bk_8_index+64-A_cp_k_bk_9_index)*64;
        A_cp_i_bk_8_index = A_cp_i_bk_8_index+64;
     }
   omp_set_num_threads(2);
   #pragma omp  parallel  
    {
    #pragma omp  for private(__FD__A_cp__n_0,__FD__A_cp__n_0_0,__FD__A_cp__n_0_0_0,__FD__A_cp__n_0_0_0_0,__FD__A_cp_0,__FD__A_cp_0_0,__FD__B_cp__n_0,__FD__B_cp__n_0_0,__FD__B_cp__n_0_0_0,__FD__B_cp__n_0_0_0_0,__FD__B_cp_0,__FD__B_cp_0_0,__FD__B_cp_0_0_0,__FD__B_cp_0_0_0_0,_A_cp__n_0_0,_A_cp_0,_B_cp__n_0_0,_B_cp__n_1_0,_B_cp__n_2_0,_B_cp__n_3_0,_B_cp_0_0,_B_cp_1_0,_B_cp_2_0,_B_cp_3_0,k,i,j,j_bk_1,j_bk_2,i_bk_3,k_bk_4,_tmp__split)
    for (j_bk_1=0; j_bk_1<N; j_bk_1+=256)
      {
         __FD__B_cp_0 = B_cp+j_bk_1*B_cp_k_bk_7_index;
         __FD__B_cp__n_0 = B_cp+j_bk_1*B_cp_k_bk_7_index;
         for (j_bk_2=0; j_bk_2<-63+min(256,N-j_bk_1); j_bk_2+=64)
           {
              __FD__B_cp__n_0_0 = __FD__B_cp__n_0;
              __FD__A_cp_0 = A_cp;
              __FD__A_cp__n_0 = A_cp;
              for (i_bk_3=0; i_bk_3<-63+N; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-1+i_bk_3; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<64; j+=4)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<64; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  vec_mov_mr_1(64+__FD__B_cp__n_0_0_0_0,reg4);
                                  vec_mov_mr_1(128+__FD__B_cp__n_0_0_0_0,reg5);
                                  vec_mov_mr_1(192+__FD__B_cp__n_0_0_0_0,reg6);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(128+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_mr_a(192+__FD__B_cp_0_0_0_0,reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg4,reg11);
                                  vec_mov_rm_1(reg4,64+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg5,reg11);
                                  vec_mov_rm_1(reg5,128+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg6,reg11);
                                  vec_mov_rm_1(reg6,192+__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*4;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*4;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<63+i_bk_3) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<64; j+=4)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<64; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  vec_mov_mr_1(64+__FD__B_cp__n_0_0_0_0,reg4);
                                  vec_mov_mr_1(128+__FD__B_cp__n_0_0_0_0,reg5);
                                  vec_mov_mr_1(192+__FD__B_cp__n_0_0_0_0,reg6);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-1+min(64,-k_bk_4+(i_bk_3+i)); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(128+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_mr_a(192+__FD__B_cp_0_0_0_0,reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-k_bk_4+(i_bk_3+i); k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg1);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+k),reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(128+k),reg9);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(192+k),reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                    }
                                  vec_div_rr(reg2,reg3);
                                  vec_div_rr(reg2,reg4);
                                  vec_div_rr(reg2,reg5);
                                  vec_div_rr(reg2,reg6);
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg4,reg11);
                                  vec_mov_rm_1(reg4,64+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg5,reg11);
                                  vec_mov_rm_1(reg5,128+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg6,reg11);
                                  vec_mov_rm_1(reg6,192+__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*4;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*4;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_9_index;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_9_index;
                }
              if (i_bk_3<N) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-1+i_bk_3; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<64; j+=4)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  vec_mov_mr_1(64+__FD__B_cp__n_0_0_0_0,reg4);
                                  vec_mov_mr_1(128+__FD__B_cp__n_0_0_0_0,reg5);
                                  vec_mov_mr_1(192+__FD__B_cp__n_0_0_0_0,reg6);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(128+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_mr_a(192+__FD__B_cp_0_0_0_0,reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg4,reg11);
                                  vec_mov_rm_1(reg4,64+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg5,reg11);
                                  vec_mov_rm_1(reg5,128+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg6,reg11);
                                  vec_mov_rm_1(reg6,192+__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*4;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*4;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<63+i_bk_3) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<64; j+=4)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  vec_mov_mr_1(64+__FD__B_cp__n_0_0_0_0,reg4);
                                  vec_mov_mr_1(128+__FD__B_cp__n_0_0_0_0,reg5);
                                  vec_mov_mr_1(192+__FD__B_cp__n_0_0_0_0,reg6);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-1+min(64,-k_bk_4+(i_bk_3+i)); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(128+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_mr_a(192+__FD__B_cp_0_0_0_0,reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-k_bk_4+(i_bk_3+i); k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg1);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+k),reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(128+k),reg9);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(192+k),reg10);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg8,reg0);
                                       vec_sub_rr(reg0,reg4);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg9,reg0);
                                       vec_sub_rr(reg0,reg5);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg10,reg0);
                                       vec_sub_rr(reg0,reg6);
                                    }
                                  vec_div_rr(reg2,reg3);
                                  vec_div_rr(reg2,reg4);
                                  vec_div_rr(reg2,reg5);
                                  vec_div_rr(reg2,reg6);
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg4,reg11);
                                  vec_mov_rm_1(reg4,64+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg5,reg11);
                                  vec_mov_rm_1(reg5,128+__FD__B_cp__n_0_0_0_0);
                                  vec_red(reg6,reg11);
                                  vec_mov_rm_1(reg6,192+__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*4;
                             __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0_0+64*4;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_9_index;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_9_index;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_k_bk_7_index;
              __FD__B_cp__n_0 = __FD__B_cp__n_0+64*B_cp_k_bk_7_index;
           }
         if (j_bk_2<min(256,N-j_bk_1)) 
           {
              __FD__B_cp__n_0_0 = __FD__B_cp__n_0;
              __FD__A_cp_0 = A_cp;
              __FD__A_cp__n_0 = A_cp;
              for (i_bk_3=0; i_bk_3<-63+N; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-1+i_bk_3; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<64; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<63+i_bk_3) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<64; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-1+min(64,-k_bk_4+(i_bk_3+i)); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-k_bk_4+(i_bk_3+i); k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg1);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                    }
                                  vec_div_rr(reg2,reg3);
                                  vec_div_rr(reg2,reg4);
                                  vec_div_rr(reg2,reg5);
                                  vec_div_rr(reg2,reg6);
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_9_index;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_9_index;
                }
              if (i_bk_3<N) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp__n_0_0 = __FD__A_cp__n_0;
                   for (k_bk_4=0; k_bk_4<-1+i_bk_3; k_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<64; k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   if (k_bk_4<63+i_bk_3) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__B_cp__n_0_0_0 = __FD__B_cp__n_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__B_cp__n_0_0_0_0 = __FD__B_cp__n_0_0_0;
                             __FD__A_cp_0_0 = __FD__A_cp_0;
                             __FD__A_cp__n_0_0_0 = __FD__A_cp__n_0_0;
                             for (i=0; i<N-i_bk_3; i+=1)
                               {
                                  vec_splat(__FD__A_cp_0_0+(64*i_bk_3+i),reg2);
                                  vec_mov_mr_1(__FD__B_cp__n_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0;
                                  for (k=0; k<-1+min(64,-k_bk_4+(i_bk_3+i)); k+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp__n_0_0_0_0,reg1);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp__n_0_0_0_0 = __FD__A_cp__n_0_0_0_0+2;
                                    }
                                  for (k=k; k<-k_bk_4+(i_bk_3+i); k+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp__n_0_0_0+k,reg1);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+k,reg7);
                                       vec_mov_rr(reg1,reg0);
                                       vec_mul_rr(reg7,reg0);
                                       vec_sub_rr(reg0,reg3);
                                    }
                                  vec_div_rr(reg2,reg3);
                                  vec_div_rr(reg2,reg4);
                                  vec_div_rr(reg2,reg5);
                                  vec_div_rr(reg2,reg6);
                                  vec_red(reg3,reg11);
                                  vec_mov_rm_1(reg3,__FD__B_cp__n_0_0_0_0);
                                  __FD__B_cp__n_0_0_0_0 = 1+__FD__B_cp__n_0_0_0_0;
                                  __FD__A_cp_0_0 = 64+__FD__A_cp_0_0;
                                  __FD__A_cp__n_0_0_0 = 64+__FD__A_cp__n_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__B_cp__n_0_0_0 = 64+__FD__B_cp__n_0_0_0;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp__n_0_0 = __FD__A_cp__n_0_0+64*64;
                     }
                   __FD__B_cp__n_0_0 = __FD__B_cp__n_0_0+64*64;
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_k_bk_9_index;
                   __FD__A_cp__n_0 = __FD__A_cp__n_0+64*A_cp_k_bk_9_index;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_k_bk_7_index;
              __FD__B_cp__n_0 = __FD__B_cp__n_0+64*B_cp_k_bk_7_index;
           }
      }
    }
   
   free(A_cp_alloc);
   B_cp_index = 0;
   for (B_cp_j_bk_6_index=0; B_cp_j_bk_6_index<-255+N; B_cp_j_bk_6_index+=256)
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<256+B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
          }
     }
   if (B_cp_j_bk_6_index<N) 
     {
        for (B_cp_j_bk_2_bk_5_index=B_cp_j_bk_6_index; B_cp_j_bk_2_bk_5_index<-63+N; B_cp_j_bk_2_bk_5_index+=64)
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                    }
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<64+B_cp_j_bk_2_bk_5_index; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_6_index+256-B_cp_k_bk_7_index)*64;
          }
        if (B_cp_j_bk_2_bk_5_index<N) 
          {
             for (B_cp_k_bk_7_index=0; B_cp_k_bk_7_index<-63+N; B_cp_k_bk_7_index+=64)
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<64+B_cp_k_bk_7_index; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
               }
             if (B_cp_k_bk_7_index<N) 
               {
                  for (j=B_cp_j_bk_2_bk_5_index; j<N; j+=1)
                    {
                       for (k=B_cp_k_bk_7_index; k<N; k+=1)
                         {
                            B[k+j*ldb] = B_cp[B_cp_index++];
                         }
                       B_cp_index = B_cp_index+(-N+(64+B_cp_k_bk_7_index))*1;
                    }
                  B_cp_index = B_cp_index+(-N+(64+B_cp_j_bk_2_bk_5_index))*64;
                  B_cp_k_bk_7_index = B_cp_k_bk_7_index+64;
               }
             B_cp_index = B_cp_index+(B_cp_j_bk_2_bk_5_index+64-B_cp_k_bk_7_index)*64;
             B_cp_j_bk_2_bk_5_index = B_cp_j_bk_2_bk_5_index+64;
          }
        B_cp_index = B_cp_index+(B_cp_j_bk_6_index+256-B_cp_j_bk_2_bk_5_index)*64;
        B_cp_j_bk_6_index = B_cp_j_bk_6_index+256;
     }
   free(B_cp_alloc);
}}


