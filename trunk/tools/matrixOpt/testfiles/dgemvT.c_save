#define max(a,b) (((a) < (b))? (b) : (a))
#define min(a,b) (((a) < (b))? (a) : (b))

#include <SSE3Dnow.h>

#include <omp.h>


void dgemvT(const int M,const int N,const double alpha,const double *A,const int lda,const double *X,const int incX,const double beta,double *Y,const int incY)
{
  int i;
  int j;
{
   int i_bk_1;
   int i_bk_2;
   int j_bk_3;
   double _A_0_0;
   double _A_1_0;
   double _A_2_0;
   double _A_3_0;
   double _X_0;
   double _Y_0;
   double _Y_1;
   double _Y_2;
   double _Y_3;
   double _tmp__split;
   double* __FD__A_0;
   double* __FD__A_0_0;
   double* __FD__A_0_0_0;
   double* __FD__A_0_0_0_0;
   double* __FD__X_0;
   double* __FD__X_0_0;
   double* __FD__Y_0;
   double* __FD__Y_0_0;
   omp_set_num_threads(2);
   #pragma omp  parallel  
    {
    #pragma omp  for private(__FD__Y_0,__FD__Y_0_0,__FD__X_0,__FD__X_0_0,__FD__A_0,__FD__A_0_0,__FD__A_0_0_0,__FD__A_0_0_0_0,_Y_0,_Y_1,_Y_2,_Y_3,_X_0,_A_0_0,_A_1_0,_A_2_0,_A_3_0,j,i,i_bk_1,i_bk_2,j_bk_3,_tmp__split)
    for (i_bk_1=0; i_bk_1<M; i_bk_1+=256)
      {
         __FD__A_0 = A+i_bk_1*lda;
         __FD__Y_0 = Y+i_bk_1;
         for (i_bk_2=0; i_bk_2<-63+min(256,M-i_bk_1); i_bk_2+=64)
           {
              __FD__A_0_0 = __FD__A_0;
              __FD__X_0 = X;
              if ((j_bk_3=0)<-63+N) 
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<64; i+=4)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        vec_mov_mr_1(1+__FD__Y_0_0,reg3);
                        vec_mov_mr_1(2+__FD__Y_0_0,reg4);
                        vec_mov_mr_1(3+__FD__Y_0_0,reg5);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        j = 0;
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mul_rr(reg0,reg2);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mul_rr(reg0,reg3);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mul_rr(reg0,reg4);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mul_rr(reg0,reg5);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=4; j<64; j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        vec_red(reg3,reg11);
                        vec_mov_rm_1(reg3,1+__FD__Y_0_0);
                        vec_red(reg4,reg11);
                        vec_mov_rm_1(reg4,2+__FD__Y_0_0);
                        vec_red(reg5,reg11);
                        vec_mov_rm_1(reg5,3+__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+4*lda;
                        __FD__Y_0_0 = __FD__Y_0_0+4;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              for (j_bk_3=64; j_bk_3<-63+N; j_bk_3+=64)
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<64; i+=4)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        vec_mov_mr_1(1+__FD__Y_0_0,reg3);
                        vec_mov_mr_1(2+__FD__Y_0_0,reg4);
                        vec_mov_mr_1(3+__FD__Y_0_0,reg5);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        for (j=0; j<64; j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        vec_red(reg3,reg11);
                        vec_mov_rm_1(reg3,1+__FD__Y_0_0);
                        vec_red(reg4,reg11);
                        vec_mov_rm_1(reg4,2+__FD__Y_0_0);
                        vec_red(reg5,reg11);
                        vec_mov_rm_1(reg5,3+__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+4*lda;
                        __FD__Y_0_0 = __FD__Y_0_0+4;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              if (j_bk_3<N) 
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<64; i+=4)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        vec_mov_mr_1(1+__FD__Y_0_0,reg3);
                        vec_mov_mr_1(2+__FD__Y_0_0,reg4);
                        vec_mov_mr_1(3+__FD__Y_0_0,reg5);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        for (j=0; j<-3+min(64,N-j_bk_3); j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=j; j<-1+min(64,N-j_bk_3); j+=2)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_mr(__FD__A_0_0_0_0+lda,reg8);
                             vec_mov_mr(__FD__A_0_0_0_0+2*lda,reg9);
                             vec_mov_mr(__FD__A_0_0_0_0+3*lda,reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=j; j<N-j_bk_3; j+=1)
                          {
                             vec_mov_mr_1(__FD__X_0+j,reg6);
                             vec_mov_mr_1(__FD__A_0_0_0+j,reg7);
                             vec_mov_mr_1(__FD__A_0_0_0+(lda+j),reg8);
                             vec_mov_mr_1(__FD__A_0_0_0+(2*lda+j),reg9);
                             vec_mov_mr_1(__FD__A_0_0_0+(3*lda+j),reg10);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             vec_mov_rr(reg8,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg3);
                             vec_mov_rr(reg9,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg4);
                             vec_mov_rr(reg10,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg5);
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        vec_red(reg3,reg11);
                        vec_mov_rm_1(reg3,1+__FD__Y_0_0);
                        vec_red(reg4,reg11);
                        vec_mov_rm_1(reg4,2+__FD__Y_0_0);
                        vec_red(reg5,reg11);
                        vec_mov_rm_1(reg5,3+__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+4*lda;
                        __FD__Y_0_0 = __FD__Y_0_0+4;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              __FD__A_0 = __FD__A_0+64*lda;
              __FD__Y_0 = __FD__Y_0+64;
           }
         if (i_bk_2<min(256,M-i_bk_1)) 
           {
              __FD__A_0_0 = __FD__A_0;
              __FD__X_0 = X;
              if ((j_bk_3=0)<-63+N) 
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<min(256-i_bk_2,-i_bk_2+(M-i_bk_1)); i+=1)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        j = 0;
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mul_rr(reg0,reg2);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=4; j<64; j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+lda;
                        __FD__Y_0_0 = 1+__FD__Y_0_0;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              for (j_bk_3=64; j_bk_3<-63+N; j_bk_3+=64)
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<min(256-i_bk_2,-i_bk_2+(M-i_bk_1)); i+=1)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        for (j=0; j<64; j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+lda;
                        __FD__Y_0_0 = 1+__FD__Y_0_0;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              if (j_bk_3<N) 
                {
                   __FD__A_0_0_0 = __FD__A_0_0;
                   __FD__Y_0_0 = __FD__Y_0;
                   for (i=0; i<min(256-i_bk_2,-i_bk_2+(M-i_bk_1)); i+=1)
                     {
                        vec_splat(&beta,reg0);
                        vec_mov_mr_1(__FD__Y_0_0,reg2);
                        __FD__A_0_0_0_0 = __FD__A_0_0_0;
                        __FD__X_0_0 = __FD__X_0;
                        for (j=0; j<-3+min(64,N-j_bk_3); j+=4)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=j; j<-1+min(64,N-j_bk_3); j+=2)
                          {
                             vec_mov_mr(__FD__X_0_0,reg6);
                             vec_mov_mr(__FD__A_0_0_0_0,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                             __FD__A_0_0_0_0 = __FD__A_0_0_0_0+2;
                             __FD__X_0_0 = __FD__X_0_0+2;
                          }
                        for (j=j; j<N-j_bk_3; j+=1)
                          {
                             vec_mov_mr_1(__FD__X_0+j,reg6);
                             vec_mov_mr_1(__FD__A_0_0_0+j,reg7);
                             vec_mov_rr(reg7,reg1);
                             vec_mul_rr(reg6,reg1);
                             vec_add_rr(reg1,reg2);
                          }
                        vec_red(reg2,reg11);
                        vec_mov_rm_1(reg2,__FD__Y_0_0);
                        __FD__A_0_0_0 = __FD__A_0_0_0+lda;
                        __FD__Y_0_0 = 1+__FD__Y_0_0;
                     }
                   __FD__A_0_0 = __FD__A_0_0+64;
                   __FD__X_0 = __FD__X_0+64;
                }
              __FD__A_0 = __FD__A_0+64*lda;
              __FD__Y_0 = __FD__Y_0+64;
           }
      }
    }
   
}}

