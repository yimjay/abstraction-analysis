#define max(a,b) (((a) < (b))? (b) : (a))
#define min(a,b) (((a) < (b))? (a) : (b))

#include <SSE3Dnow.h>

#include <stdlib.h>
#include <omp.h>


void dgemm(const int M,const int N,const int K,const double alpha,const double *A,const int lda,const double *B,const int ldb,const double beta,double *C,const int ldc)
{
  int i;
  int j;
  int l;
{
   int j_bk_1;
   int j_bk_2;
   int i_bk_3;
   int l_bk_4;
   int B_cp_j_bk_5_index;
   int B_cp_l_bk_6_index;
   int B_cp_index;
   double* B_cp_alloc;
   double* B_cp;
   int A_cp_i_bk_7_index;
   int A_cp_l_bk_8_index;
   int A_cp_index;
   double* A_cp_alloc;
   double* A_cp;
   double _B_cp_0_0;
   double _B_cp_1_0;
   double _A_cp_0_0;
   double _A_cp_1_0;
   double _C_0_0;
   double _C_0_1;
   double _C_1_0;
   double _C_1_1;
   double _tmp__split;
   double* __FD__B_cp_0;
   double* __FD__B_cp_0_0;
   double* __FD__B_cp_0_0_0;
   double* __FD__B_cp_0_0_0_0;
   double* __FD__A_cp_0;
   double* __FD__A_cp_0_0;
   double* __FD__A_cp_0_0_0;
   double* __FD__A_cp_0_0_0_0;
   double* __FD__C_0;
   double* __FD__C_0_0;
   double* __FD__C_0_0_0;
   double* __FD__C_0_0_0_0;
   omp_set_num_threads(2);
   #pragma omp  parallel  
    {
    #pragma omp  for private(__FD__C_0,__FD__C_0_0,__FD__C_0_0_0,__FD__C_0_0_0_0,__FD__A_cp_0,__FD__A_cp_0_0,__FD__A_cp_0_0_0,__FD__A_cp_0_0_0_0,__FD__B_cp_0,__FD__B_cp_0_0,__FD__B_cp_0_0_0,__FD__B_cp_0_0_0_0,_C_0_0,_C_0_1,_C_1_0,_C_1_1,_A_cp_0_0,_A_cp_1_0,_B_cp_0_0,_B_cp_1_0,l,i,j,j_bk_1,j_bk_2,i_bk_3,l_bk_4,B_cp_j_bk_5_index,B_cp_l_bk_6_index,B_cp_index,B_cp,B_cp_alloc,A_cp_i_bk_7_index,A_cp_l_bk_8_index,A_cp_index,A_cp,A_cp_alloc,_tmp__split)
    for (j_bk_1=0; j_bk_1<N; j_bk_1+=256)
      {
         B_cp_alloc=(double*)malloc((64*((63+min(256,N-j_bk_1))/64)*(64*((63+K)/64))+(1<<16))*sizeof(double));
         B_cp=(double*)((size_t)B_cp_alloc + (1 << 16) >> 16 << 16);
         B_cp_index = 0;
         for (B_cp_j_bk_5_index=0; B_cp_j_bk_5_index<-63+min(256,N-j_bk_1); B_cp_j_bk_5_index+=64)
           {
              for (B_cp_l_bk_6_index=0; B_cp_l_bk_6_index<-63+K; B_cp_l_bk_6_index+=64)
                {
                   for (j=B_cp_j_bk_5_index; j<64+B_cp_j_bk_5_index; j+=1)
                     {
                        for (l=B_cp_l_bk_6_index; l<64+B_cp_l_bk_6_index; l+=1)
                          {
                             B_cp[B_cp_index++] = B[l+(j_bk_1*ldb+j*ldb)];
                          }
                     }
                }
              if (B_cp_l_bk_6_index<K) 
                {
                   for (j=B_cp_j_bk_5_index; j<64+B_cp_j_bk_5_index; j+=1)
                     {
                        for (l=B_cp_l_bk_6_index; l<K; l+=1)
                          {
                             B_cp[B_cp_index++] = B[l+(j_bk_1*ldb+j*ldb)];
                          }
                        B_cp_index = B_cp_index+(-K+(64+B_cp_l_bk_6_index));
                     }
                   B_cp_l_bk_6_index = 64+B_cp_l_bk_6_index;
                }
           }
         if (B_cp_j_bk_5_index<min(256,N-j_bk_1)) 
           {
              for (B_cp_l_bk_6_index=0; B_cp_l_bk_6_index<-63+K; B_cp_l_bk_6_index+=64)
                {
                   for (j=B_cp_j_bk_5_index; j<min(256,N-j_bk_1); j+=1)
                     {
                        for (l=B_cp_l_bk_6_index; l<64+B_cp_l_bk_6_index; l+=1)
                          {
                             B_cp[B_cp_index++] = B[l+(j_bk_1*ldb+j*ldb)];
                          }
                     }
                   B_cp_index = B_cp_index+(64*-min(256,N-j_bk_1)+(4096+64*B_cp_j_bk_5_index));
                }
              if (B_cp_l_bk_6_index<K) 
                {
                   for (j=B_cp_j_bk_5_index; j<min(256,N-j_bk_1); j+=1)
                     {
                        for (l=B_cp_l_bk_6_index; l<K; l+=1)
                          {
                             B_cp[B_cp_index++] = B[l+(j_bk_1*ldb+j*ldb)];
                          }
                        B_cp_index = B_cp_index+(-K+(64+B_cp_l_bk_6_index));
                     }
                   B_cp_index = B_cp_index+(64*-min(256,N-j_bk_1)+(4096+64*B_cp_j_bk_5_index));
                   B_cp_l_bk_6_index = 64+B_cp_l_bk_6_index;
                }
              B_cp_index = B_cp_index+(64*-B_cp_l_bk_6_index+(4096+64*B_cp_j_bk_5_index));
              B_cp_j_bk_5_index = 64+B_cp_j_bk_5_index;
           }
         A_cp_alloc=(double*)malloc((64*((63+M)/64)*(64*((63+K)/64))+(1<<16))*sizeof(double));
         A_cp=(double*)((size_t)A_cp_alloc + (1 << 16) >> 16 << 16);
         A_cp_index = 0;
         for (A_cp_i_bk_7_index=0; A_cp_i_bk_7_index<-63+M; A_cp_i_bk_7_index+=64)
           {
              for (A_cp_l_bk_8_index=0; A_cp_l_bk_8_index<-63+K; A_cp_l_bk_8_index+=64)
                {
                   for (i=A_cp_i_bk_7_index; i<64+A_cp_i_bk_7_index; i+=1)
                     {
                        for (l=A_cp_l_bk_8_index; l<64+A_cp_l_bk_8_index; l+=1)
                          {
                             A_cp[A_cp_index++] = alpha*A[i+l*lda];
                          }
                     }
                }
              if (A_cp_l_bk_8_index<K) 
                {
                   for (i=A_cp_i_bk_7_index; i<64+A_cp_i_bk_7_index; i+=1)
                     {
                        for (l=A_cp_l_bk_8_index; l<K; l+=1)
                          {
                             A_cp[A_cp_index++] = alpha*A[i+l*lda];
                          }
                        A_cp_index = A_cp_index+(-K+(64+A_cp_l_bk_8_index));
                     }
                   A_cp_l_bk_8_index = 64+A_cp_l_bk_8_index;
                }
           }
         if (A_cp_i_bk_7_index<M) 
           {
              for (A_cp_l_bk_8_index=0; A_cp_l_bk_8_index<-63+K; A_cp_l_bk_8_index+=64)
                {
                   for (i=A_cp_i_bk_7_index; i<M; i+=1)
                     {
                        for (l=A_cp_l_bk_8_index; l<64+A_cp_l_bk_8_index; l+=1)
                          {
                             A_cp[A_cp_index++] = alpha*A[i+l*lda];
                          }
                     }
                   A_cp_index = A_cp_index+(64*-M+(4096+64*A_cp_i_bk_7_index));
                }
              if (A_cp_l_bk_8_index<K) 
                {
                   for (i=A_cp_i_bk_7_index; i<M; i+=1)
                     {
                        for (l=A_cp_l_bk_8_index; l<K; l+=1)
                          {
                             A_cp[A_cp_index++] = alpha*A[i+l*lda];
                          }
                        A_cp_index = A_cp_index+(-K+(64+A_cp_l_bk_8_index));
                     }
                   A_cp_index = A_cp_index+(64*-M+(4096+64*A_cp_i_bk_7_index));
                   A_cp_l_bk_8_index = 64+A_cp_l_bk_8_index;
                }
              A_cp_index = A_cp_index+(64*-A_cp_l_bk_8_index+(4096+64*A_cp_i_bk_7_index));
              A_cp_i_bk_7_index = 64+A_cp_i_bk_7_index;
           }
         __FD__B_cp_0 = B_cp;
         __FD__C_0 = C+j_bk_1*ldc;
         for (j_bk_2=0; j_bk_2<-63+min(256,N-j_bk_1); j_bk_2+=64)
           {
              __FD__A_cp_0 = A_cp;
              __FD__C_0_0 = __FD__C_0;
              for (i_bk_3=0; i_bk_3<-63+M; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   if ((l_bk_4=0)<-63+K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+(1+ldc),reg5);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  l = 0;
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mul_rr(reg0,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mul_rr(reg0,reg3);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mul_rr(reg0,reg4);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mul_rr(reg0,reg5);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=4; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  vec_red(reg5,reg10);
                                  vec_mov_rm_1(reg5,__FD__C_0_0_0_0+(1+ldc));
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   for (l_bk_4=64; l_bk_4<-63+K; l_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+(1+ldc),reg5);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  vec_red(reg5,reg10);
                                  vec_mov_rm_1(reg5,__FD__C_0_0_0_0+(1+ldc));
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   if (l_bk_4<K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+(1+ldc),reg5);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<-3+min(64,K-l_bk_4); l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<-1+min(64,K-l_bk_4); l+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<K-l_bk_4; l+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+l,reg6);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+(64+l),reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+l,reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+l),reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg5);
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  vec_red(reg5,reg10);
                                  vec_mov_rm_1(reg5,__FD__C_0_0_0_0+(1+ldc));
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_l_bk_8_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              if (i_bk_3<M) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   if ((l_bk_4=0)<-63+K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  l = 0;
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mul_rr(reg0,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mul_rr(reg0,reg4);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=4; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   for (l_bk_4=64; l_bk_4<-63+K; l_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   if (l_bk_4<K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<64; j+=2)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(__FD__C_0_0_0_0+ldc,reg4);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<-3+min(64,K-l_bk_4); l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<-1+min(64,K-l_bk_4); l+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_mr_a(64+__FD__B_cp_0_0_0_0,reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<K-l_bk_4; l+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+l,reg6);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+l,reg8);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+(64+l),reg9);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg9,reg1);
                                       vec_add_rr(reg1,reg4);
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg4,reg10);
                                  vec_mov_rm_1(reg4,__FD__C_0_0_0_0+ldc);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = __FD__B_cp_0_0_0+64*2;
                             __FD__C_0_0_0 = __FD__C_0_0_0+2*ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_l_bk_8_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_l_bk_6_index;
              __FD__C_0 = __FD__C_0+64*ldc;
           }
         if (j_bk_2<min(256,N-j_bk_1)) 
           {
              __FD__A_cp_0 = A_cp;
              __FD__C_0_0 = __FD__C_0;
              for (i_bk_3=0; i_bk_3<-63+M; i_bk_3+=64)
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   if ((l_bk_4=0)<-63+K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  l = 0;
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mul_rr(reg0,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mul_rr(reg0,reg3);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=4; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   for (l_bk_4=64; l_bk_4<-63+K; l_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   if (l_bk_4<K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<64; i+=2)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  vec_mov_mr_1(1+__FD__C_0_0_0_0,reg3);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<-3+min(64,K-l_bk_4); l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<-1+min(64,K-l_bk_4); l+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(64+__FD__A_cp_0_0_0_0,reg7);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<K-l_bk_4; l+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+l,reg6);
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+(64+l),reg7);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+l,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       vec_mov_rr(reg7,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg3);
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  vec_red(reg3,reg10);
                                  vec_mov_rm_1(reg3,1+__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = __FD__A_cp_0_0_0+64*2;
                                  __FD__C_0_0_0_0 = __FD__C_0_0_0_0+2;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_l_bk_8_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              if (i_bk_3<M) 
                {
                   __FD__B_cp_0_0 = __FD__B_cp_0;
                   __FD__A_cp_0_0 = __FD__A_cp_0;
                   if ((l_bk_4=0)<-63+K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  l = 0;
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mul_rr(reg0,reg2);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=4; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   for (l_bk_4=64; l_bk_4<-63+K; l_bk_4+=64)
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<64; l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   if (l_bk_4<K) 
                     {
                        __FD__B_cp_0_0_0 = __FD__B_cp_0_0;
                        __FD__C_0_0_0 = __FD__C_0_0;
                        for (j=0; j<min(256-j_bk_2,-j_bk_2+(N-j_bk_1)); j+=1)
                          {
                             __FD__A_cp_0_0_0 = __FD__A_cp_0_0;
                             __FD__C_0_0_0_0 = __FD__C_0_0_0;
                             for (i=0; i<M-i_bk_3; i+=1)
                               {
                                  vec_splat(&beta,reg0);
                                  vec_mov_mr_1(__FD__C_0_0_0_0,reg2);
                                  __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0;
                                  __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0;
                                  for (l=0; l<-3+min(64,K-l_bk_4); l+=4)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<-1+min(64,K-l_bk_4); l+=2)
                                    {
                                       vec_mov_mr_a(__FD__A_cp_0_0_0_0,reg6);
                                       vec_mov_mr_a(__FD__B_cp_0_0_0_0,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                       __FD__B_cp_0_0_0_0 = __FD__B_cp_0_0_0_0+2;
                                       __FD__A_cp_0_0_0_0 = __FD__A_cp_0_0_0_0+2;
                                    }
                                  for (l=l; l<K-l_bk_4; l+=1)
                                    {
                                       vec_mov_mr_1(__FD__A_cp_0_0_0+l,reg6);
                                       vec_mov_mr_1(__FD__B_cp_0_0_0+l,reg8);
                                       vec_mov_rr(reg6,reg1);
                                       vec_mul_rr(reg8,reg1);
                                       vec_add_rr(reg1,reg2);
                                    }
                                  vec_red(reg2,reg10);
                                  vec_mov_rm_1(reg2,__FD__C_0_0_0_0);
                                  __FD__A_cp_0_0_0 = 64+__FD__A_cp_0_0_0;
                                  __FD__C_0_0_0_0 = 1+__FD__C_0_0_0_0;
                               }
                             __FD__B_cp_0_0_0 = 64+__FD__B_cp_0_0_0;
                             __FD__C_0_0_0 = __FD__C_0_0_0+ldc;
                          }
                        __FD__B_cp_0_0 = __FD__B_cp_0_0+64*64;
                        __FD__A_cp_0_0 = __FD__A_cp_0_0+64*64;
                     }
                   __FD__A_cp_0 = __FD__A_cp_0+64*A_cp_l_bk_8_index;
                   __FD__C_0_0 = __FD__C_0_0+64;
                }
              __FD__B_cp_0 = __FD__B_cp_0+64*B_cp_l_bk_6_index;
              __FD__C_0 = __FD__C_0+64*ldc;
           }
         free(A_cp_alloc);
         free(B_cp_alloc);
      }
    }
   
}}

